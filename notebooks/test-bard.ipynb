{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:39.697125Z",
     "start_time": "2022-07-28T22:27:39.683710Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from count_syllables import count_syllables\n",
    "from nltk.corpus import cmudict\n",
    "from spacy.lang.en import English\n",
    "from string import punctuation\n",
    "import json\n",
    "import logging\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import sys\n",
    "import tracery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.disable(\n",
    "    logging.CRITICAL\n",
    ")  # comment out to enable debugging messages logging.basicConfig(level=logging.DEBUG, format='%(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/bean/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"cmudict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/kw6252cj3lqgftmsg97nw4z80000gn/T/ipykernel_17985/2263952448.py:4: DeprecationWarning: The *random* parameter to shuffle() has been deprecated\n",
      "since Python 3.9 and will be removed in a subsequent version.\n",
      "  lesson_list1 = random.shuffle(lesson_list, random)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000003?line=1'>2</a>\u001b[0m     lesson \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000003?line=2'>3</a>\u001b[0m     lesson_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(lesson\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000003?line=3'>4</a>\u001b[0m     lesson_list1 \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mshuffle(lesson_list, random)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000003?line=4'>5</a>\u001b[0m     lesson_list2 \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mshuffle(lesson_list)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000003?line=5'>6</a>\u001b[0m lesson_list1\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/random.py:403\u001b[0m, in \u001b[0;36mRandom.shuffle\u001b[0;34m(self, x, random)\u001b[0m\n\u001b[1;32m    400\u001b[0m floor \u001b[39m=\u001b[39m _floor\n\u001b[1;32m    401\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(x))):\n\u001b[1;32m    402\u001b[0m     \u001b[39m# pick an element in x[:i+1] with which to exchange x[i]\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     j \u001b[39m=\u001b[39m floor(random() \u001b[39m*\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[1;32m    404\u001b[0m     x[i], x[j] \u001b[39m=\u001b[39m x[j], x[i]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "with open(\"../haikuifier/lesson.txt\") as f:\n",
    "    lesson = f.read()\n",
    "    lesson_list = list(lesson.replace(\" \", \", \"))\n",
    "lesson_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lesson_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000071?line=0'>1</a>\u001b[0m lesson \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49mshuffle(lesson)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/random.py:394\u001b[0m, in \u001b[0;36mRandom.shuffle\u001b[0;34m(self, x, random)\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(x))):\n\u001b[1;32m    392\u001b[0m         \u001b[39m# pick an element in x[:i+1] with which to exchange x[i]\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         j \u001b[39m=\u001b[39m randbelow(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 394\u001b[0m         x[i], x[j] \u001b[39m=\u001b[39m x[j], x[i]\n\u001b[1;32m    395\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    396\u001b[0m     _warn(\u001b[39m'\u001b[39m\u001b[39mThe *random* parameter to shuffle() has been deprecated\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    397\u001b[0m           \u001b[39m'\u001b[39m\u001b[39msince Python 3.9 and will be removed in a subsequent \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    398\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mversion.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    399\u001b[0m           \u001b[39mDeprecationWarning\u001b[39;00m, \u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "lesson = random.shuffle(lesson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/kw6252cj3lqgftmsg97nw4z80000gn/T/ipykernel_63382/169025837.py:5: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  haikuraw = haikuraw.str.replace(\".\", \"\")\n"
     ]
    }
   ],
   "source": [
    "haikucorp = lesson\n",
    "haikuraw = haikucorp.str.replace(\"/\", \"\")\n",
    "haikuraw = haikucorp.str.replace(\"/\", \"\")\n",
    "haikuraw = haikuraw.str.replace(\"  \", \" \")\n",
    "haikuraw = haikuraw.str.replace(\".\", \"\")\n",
    "rawlist = haikuraw.to_list()\n",
    "string = \" \".join(rawlist)\n",
    "with open(\"haikucorpus.txt\", \"w\") as outfile:\n",
    "    outfile.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"train.txt\", \"lesson.txt\"]\n",
    "with open(\"trainer.txt\", \"w\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_file(file):\n",
    "    \"\"\"Return text file as string.\"\"\"\n",
    "    with open(file) as f:\n",
    "        raw_haiku = f.read()\n",
    "        return raw_haiku\n",
    "\n",
    "\n",
    "def prep_training(raw_haiku):\n",
    "    \"\"\"Load string, remove newline, split words on spaces, and return list.\"\"\"\n",
    "    corpus = raw_haiku.replace(\"\\n\", \" \").split()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_word_to_word(corpus):\n",
    "    \"\"\"Load list & use dictionary to map word to word that follows\"\"\"\n",
    "    limit = len(corpus) - 1\n",
    "    dict1_to_1 = defaultdict(list)\n",
    "    for index, word in enumerate(corpus):\n",
    "        if index < limit:\n",
    "            suffix = corpus[index + 1]\n",
    "            dict1_to_1[word].append(suffix)\n",
    "    logging.debug('map_word_to_word results for \"sake\" = %s\\n', dict1_to_1[\"sake\"])\n",
    "    return dict1_to_1\n",
    "\n",
    "\n",
    "def map_2_words_to_word(corpus):\n",
    "    \"\"\"Load list and use dictionary to map word-pair to trailing word.\"\"\"\n",
    "    limit = len(corpus) - 2\n",
    "    dict2_2_1 = defaultdict(list)\n",
    "    for index, word in enumerate(corpus):\n",
    "        if index < limit:\n",
    "            key = word + \" \" + corpus[index + 1]\n",
    "            suffix = corpus[index + 2]\n",
    "            dict2_2_1[key].append(suffix)\n",
    "    logging.debug(\n",
    "        'map_2_words_to_word results for \"sake jug\" = %s\\n', dict2_2_1[\"sake jug\"]\n",
    "    )\n",
    "    return dict2_2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_word(corpus):\n",
    "    \"\"\"Return random word and syllable count from training corpus.\"\"\"\n",
    "    word = random.choice(corpus)\n",
    "    num_syls = count_syllables(word)\n",
    "    if num_syls > 4:\n",
    "        random_word(corpus)\n",
    "    else:\n",
    "        logging.debug(\"random word and syllables = %s %s\\n\", word, num_syls)\n",
    "        return (word, num_syls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_after_single(prefix, suffix_map_1, current_syls, target_syls):\n",
    "    \"\"\"Return all acceptable words in a corpus that follow a single word\"\"\"\n",
    "    accepted_words = []\n",
    "    suffixes = suffix_map_1.get(prefix)\n",
    "    if suffixes != None:\n",
    "        for candidate in suffixes:\n",
    "            num_syls = count_syllables(candidate)\n",
    "            if current_syls + num_syls <= target_syls:\n",
    "                accepted_words.append(candidate)\n",
    "    logging.debug('accepted words after \"%s\" = %s\\n', prefix, set(accepted_words))\n",
    "    return accepted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_after_double(prefix, suffix_map_2, current_syls, target_syls):\n",
    "\n",
    "    \"\"\"Return all acceptable words in a corpus that follow a word pair.\"\"\"\n",
    "\n",
    "    accepted_words = []\n",
    "    suffixes = suffix_map_2.get(prefix)\n",
    "    if suffixes != None:\n",
    "        for candidate in suffixes:\n",
    "            num_syls = count_syllables(candidate)\n",
    "            if current_syls + num_syls <= target_syls:\n",
    "                accepted_words.append(candidate)\n",
    "    logging.debug('accepted words after \"%s\" = %s\\n', prefix, set(accepted_words))\n",
    "    return accepted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line, target_syls):\n",
    "    \"\"\"Build a haiku line from a training corpus and return it.\"\"\"\n",
    "    line = \"2/3\"\n",
    "    line_syls = 0\n",
    "    current_line = []\n",
    "    if len(end_prev_line) == 0:  # first line\n",
    "        line = \"1\"\n",
    "        word, num_syls = random_word(corpus)\n",
    "        current_line.append(word)\n",
    "        line_syls += num_syls\n",
    "        word_choices = word_after_single(word, suffix_map_1, line_syls, target_syls)\n",
    "\n",
    "        while len(word_choices) == 0:\n",
    "            prefix = random.choice(corpus)\n",
    "            logging.debug(\"new random prefix = %s %s\", word, num_syls)\n",
    "            word_choices = word_after_single(\n",
    "                prefix, suffix_map_1, line_syls, target_syls\n",
    "            )\n",
    "        word = random.choice(word_choices)\n",
    "        num_syls = count_syllables(word)\n",
    "        logging.debug(\"word and syllables = %s %s\", word, num_syls)\n",
    "        line_syls += num_syls\n",
    "        current_line.append(word)\n",
    "        if line_syls == target_syls:\n",
    "            end_prev_line.extend(current_line[-2:])\n",
    "            return current_line, end_prev_line\n",
    "    else:  # lines 2 + 3\n",
    "        current_line.extend(end_prev_line)\n",
    "\n",
    "    while True:\n",
    "        logging.debug(\"line = %s\\n\", line)\n",
    "        prefix = current_line[-2] + \" \" + current_line[-1]\n",
    "        word_choices = word_after_double(prefix, suffix_map_2, line_syls, target_syls)\n",
    "\n",
    "        while len(word_choices) == 0:\n",
    "            prefix = random.choice(corpus)\n",
    "            logging.debug(\"new random prefix = %s %s\", word, num_syls)\n",
    "            word_choices = word_after_double(\n",
    "                prefix, suffix_map_2, line_syls, target_syls\n",
    "            )\n",
    "\n",
    "        word = random.choice(word_choices)\n",
    "        num_syls = count_syllables(word)\n",
    "        logging.debug(\"word and syllables = %s %s\", word, num_syls)\n",
    "\n",
    "        if line_syls + num_syls > target_syls:\n",
    "            continue\n",
    "        elif line_syls + num_syls < target_syls:\n",
    "            current_line.append(word)\n",
    "            line_syls += num_syls\n",
    "        elif line_syls + num_syls == target_syls:\n",
    "            current_line.append(word)\n",
    "            break\n",
    "\n",
    "    end_prev_line = []\n",
    "    end_prev_line.extend(current_line[-2:])\n",
    "\n",
    "    if line == \"1\":\n",
    "        final_line = current_line[:]\n",
    "    else:\n",
    "        final_line = current_line[2:]\n",
    "\n",
    "    return final_line, end_prev_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "    A N-monkeys at N-typewriters for N-millenia...\n",
      "    or one computer piped full of garbage...\n",
      "    can sometimes produce haiku.\n",
      "\n",
      "\n",
      "            \n",
      "            TMZ Haiku Generator\n",
      "            \n",
      "            0 - QUIT — I'm done having fun.\n",
      "            1 - Generate a haiku\n",
      "            2 - Regenerate Line 2\n",
      "            3 - Regenerate Line 3\n",
      "            \n",
      "            \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute '_comparable'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=80'>81</a>\u001b[0m     \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mPress the RETURN key to exit, friend.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=83'>84</a>\u001b[0m     main()\n",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb Cell 13\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=36'>37</a>\u001b[0m     sys\u001b[39m.\u001b[39mexit()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=38'>39</a>\u001b[0m     \u001b[39m# generate full haiku\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=39'>40</a>\u001b[0m \u001b[39melif\u001b[39;00m choice \u001b[39m==\u001b[39;49m tqdm(\u001b[39m\"\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=40'>41</a>\u001b[0m     final \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/test-bard.ipynb#ch0000012?line=41'>42</a>\u001b[0m     end_prev_line \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/tqdm/utils.py:75\u001b[0m, in \u001b[0;36mComparable.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_comparable \u001b[39m==\u001b[39m other\u001b[39m.\u001b[39;49m_comparable\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_comparable'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Give user choice of building a haiku or modifying an existing haiku\"\"\"\n",
    "    intro = \"\"\"\\n\n",
    "    A N-monkeys at N-typewriters for N-millenia...\n",
    "    or one computer piped full of garbage...\n",
    "    can sometimes produce haiku.\\n\"\"\"\n",
    "    print(\"{}\".format(intro))\n",
    "    \n",
    "    raw_haiku = load_training_file(\"trainer.txt\")\n",
    "    corpus = prep_training(raw_haiku)\n",
    "    suffix_map_1 = map_word_to_word(corpus)\n",
    "    suffix_map_2 = map_2_words_to_word(corpus)\n",
    "    final = []\n",
    "    \n",
    "    choice = None\n",
    "    while choice != \"0\":\n",
    "        \n",
    "        print(\n",
    "            \"\"\"\n",
    "            \n",
    "            TMZ Haiku Generator\n",
    "            \n",
    "            0 - QUIT — I'm done having fun.\n",
    "            1 - Generate a haiku\n",
    "            2 - Regenerate Line 2\n",
    "            3 - Regenerate Line 3\n",
    "            \n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        choice = input(\"Choice: \")\n",
    "        print()\n",
    "        \n",
    "        # exit\n",
    "        if choice == \"0\":\n",
    "            print(\"Coward.\")\n",
    "            sys.exit()\n",
    "            \n",
    "            # generate full haiku\n",
    "        elif choice == tqdm(\"1\"):\n",
    "            final = []\n",
    "            end_prev_line = []\n",
    "            first_line, end_prev_line1 = haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line, 5)\n",
    "            final.append(first_line)\n",
    "            line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line1, 7)\n",
    "            final.append(line)\n",
    "            line, end_prev_line3 = haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line2, 5)\n",
    "            final.append(line)\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            if not final:\n",
    "                print(\"You've gotta generate a haiku first, silly. Try Option 1.\")\n",
    "                continue\n",
    "            else:\n",
    "                line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line1, 7)\n",
    "                final[1] = line\n",
    "                \n",
    "            # Regenerate line 3\n",
    "        elif choice == \"3\":\n",
    "            if not final:\n",
    "                print(\"I can't regenerate nothing, doofus. Try Option 1.\")\n",
    "                continue\n",
    "            else:\n",
    "                line, end_prev_line2 = haiku_line(suffix_map_1, suffix_map_2, corpus, end_prev_line_2, 5)\n",
    "                final[2] = line\n",
    "                \n",
    "        # Invalid choice\n",
    "        else:\n",
    "            print(\"\\nYou need to pretend that your keyboard only has five keys: 0 1 2 3 and RETURN. Try pressing a cardinal number and then RETURN on your five-keyed keyboard. Whatever you just pushed isn't a choice I can understand.\", file=sys.stderr)\n",
    "            continue\n",
    "        \n",
    "        #display results\n",
    "        print(print(\"First line = \", end=\"\"))\n",
    "        print(' '.join(final[0]), file=sys.stderr)\n",
    "        print(\"Second line = \", end=\"\")\n",
    "        print(\" \".join(final[1]), file=sys.stderr)\n",
    "        print(\"Third line = \", end=\"\")\n",
    "        print(\" \".join(final[2]), file=sys.stderr)\n",
    "        print()\n",
    "        \n",
    "    input(\"\\n\\nPress the RETURN key to exit, friend.\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:41.756792Z",
     "start_time": "2022-07-28T22:27:40.570558Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp_en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:43.907362Z",
     "start_time": "2022-07-28T22:27:43.843959Z"
    }
   },
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('../headlines.csv')\n",
    "headline_list = headlines.headline.to_list()\n",
    "headline_str = str(headline_list)\n",
    "doc = nlp(headline_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:45.490563Z",
     "start_time": "2022-07-28T22:27:45.473425Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "words = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)\n",
    "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
    "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
    "adjs = [w for w in words if w.pos_ == \"ADJ\"]\n",
    "advs = [w for w in words if w.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:47.406805Z",
     "start_time": "2022-07-28T22:27:47.397338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of —\n",
      "sentences: 121\n",
      "words: 1255\n",
      "noun chunks: 379\n",
      "entities: 185\n",
      "- - -\n",
      "nouns: 91\n",
      "verbs: 128\n",
      "adjs: 17\n",
      "advs: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"len of —\")\n",
    "print(f\"sentences: {len(sentences)}\")\n",
    "print(f\"words: {len(words)}\")\n",
    "print(f\"noun chunks: {len(noun_chunks)}\")\n",
    "print(f\"entities: {len(entities)}\")\n",
    "print(\"- - -\")\n",
    "print(f\"nouns: {len(nouns)}\")\n",
    "print(f\"verbs: {len(verbs)}\")\n",
    "print(f\"adjs: {len(adjs)}\")\n",
    "print(f\"advs: {len(advs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:28:00.162398Z",
     "start_time": "2022-07-28T22:28:00.156358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-Game Brawl\n",
      "\n",
      "IG Model Gena Tew AIDS\n",
      "\n",
      "\", ' Celebrity Scramble Guess Who! ', ' Jake Paul Hasim Rahman Jr.\n",
      "\n",
      "Hours\n",
      "\n",
      "', ' Ukraine Prez Volodymyr Zelensky Strong Support\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for item in random.sample(noun_chunks, 5):\n",
    "    print(item.text.strip().replace(\"\\n\", \" \"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_strs = [item.text for item in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\'I Think It\\'s Going To Get Done\\' \", \\' July 2022 Hot Shots Bae Watch Alert! \\', \" HEROIC PIZZA DELIVERY',\n",
       " '\", \\' Britney Spears Will Not Be Deposed, Judge Rules \\', \\' Jeff Bezos Parents Buy $34M Mega Mansion in FL \\', \" Joey Chestnut Downs 44 Cane\\'s Chicken Fingers In Five Minutes!!! ...',\n",
       " \"For Bashing Decision overturning Roe v. Wade ', ' Bob Dylan Accuser Drops Sex Abuse Suit ', ' Jeff Ross Jon Stewart Feels the Pain of Veterans ...\",\n",
       " 'Bare at DT\\'s Golf Course \", \\' China Rocket Debris Expected To Rain Down On Earth ... AGAIN!!!',\n",
       " '\\', \\' Taco Bell Employee Allegedly Throws Boiling Hot Water at Customers\\', \" Brittney Griner Sends Message To Wife In Court ... \\'Good Luck',\n",
       " 'All These People in Red!!!',\n",
       " '\\', \" Britney Spears My Book is Done!!! ...',\n",
       " '\", \\' Luci Fang Sink Your Teeth Into Her Hot Shots!',\n",
       " 'Former Cops Get Prison Time ...',\n",
       " 'Wifey Likes to Party Too!!!']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(sentence_strs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dancers\n",
      "Pain\n",
      "Biggie\n",
      "Crowd\n",
      "Drago\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(nouns, 5): # change \"nouns\" to \"verbs\" or \"adjs\" or \"advs\" to sample from those lists!\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
    "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter([w.text for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count['Britney']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 17),\n",
       " ('to', 14),\n",
       " ('For', 13),\n",
       " ('To', 12),\n",
       " ('at', 11),\n",
       " ('My', 10),\n",
       " ('Star', 9),\n",
       " ('for', 9),\n",
       " ('of', 7),\n",
       " ('In', 7)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"words.txt\", \"w\") as fh:\n",
    "    fh.write(\"\\n\".join([w.text for w in words]))\n",
    "shutil.move(\"words.txt\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves a list of spacy values to a text file in ../data\n",
    "def save_spacy_list(filename, t):\n",
    "    with open(filename, \"w\") as fh:\n",
    "        fh.write(\"\\n\".join([item.text for item in t]))\n",
    "        ## If file exists, delete it ##\n",
    "    path = \"f'..data/{filename}'\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    else:## Show an error ##\n",
    "        print(f\"Error: {path} file not found\")\n",
    "    shutil.move(filename, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_values = [\n",
    "    \"noun_chunks\",\n",
    "    \"entities\",\n",
    "    \"words\",\n",
    "    \"adjs\",\n",
    "    \"advs\",\n",
    "    \"verbs\",\n",
    "    \"nouns\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: f'..data/{filename}' file not found\n",
      "Error: f'..data/{filename}' file not found\n",
      "Error: f'..data/{filename}' file not found\n",
      "Error: f'..data/{filename}' file not found\n",
      "Error: f'..data/{filename}' file not found\n",
      "Error: f'..data/{filename}' file not found\n",
      "Error: f'..data/{filename}' file not found\n"
     ]
    }
   ],
   "source": [
    "save_spacy_list(\"words.txt\", words)\n",
    "save_spacy_list(\"noun_chunks.txt\", noun_chunks)\n",
    "save_spacy_list(\"entities.txt\", entities)\n",
    "save_spacy_list(\"adjs.txt\", adjs)\n",
    "save_spacy_list(\"advs.txt\", advs)\n",
    "save_spacy_list(\"verbs.txt\", verbs)\n",
    "save_spacy_list(\"nouns.txt\", nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_counter_tsv(filename, counter, limit=1000):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(\"key\\tvalue\\n\")\n",
    "        for item, count in counter.most_common():\n",
    "            outfile.write(item.strip() + \"\\t\" + str(count) + \"\\n\")\n",
    "    shutil.move(filename, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_counter_tsv(\"100_common_words.tsv\", word_count, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_counter = Counter([e.text.lower() for e in people])\n",
    "save_counter_tsv(\"people_count.tsv\", people_counter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During → during\n",
      "Tryout → Tryout\n",
      "Prison → Prison\n",
      "Fraud → Fraud\n",
      "Disrespectful → disrespectful\n",
      "Savarino → Savarino\n",
      "Being → be\n",
      "The → the\n",
      "at → at\n",
      "Too → Too\n",
      "Bday → Bday\n",
      "Beyonce → Beyonce\n"
     ]
    }
   ],
   "source": [
    "for word in random.sample(words, 12):\n",
    "    print(word.text, \"→\", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideas\n",
      "to\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "sentence = random.choice(sentences)\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MORE / ADJ / JJR\n",
      "Hobby / PROPN / NNP\n",
      "WWE / PROPN / NNP\n",
      "Weight / PROPN / NNP\n",
      "Single / PROPN / NNP\n",
      "Her / PRON / PRP$\n",
      "Selfie / PROPN / NNP\n",
      "a / DET / DT\n",
      "Ukrainian / PROPN / NNP\n",
      "Sample / PROPN / NNP\n",
      "Days / NOUN / NNS\n",
      "Ric / PROPN / NNP\n",
      "You / PRON / PRP\n",
      "Too / ADV / RB\n",
      "Market / NOUN / NN\n",
      "In / ADP / IN\n",
      "To / ADP / IN\n",
      "Frigin / PROPN / NNP\n",
      "Old / PROPN / NNP\n",
      "It / PRON / PRP\n",
      "Shows / VERB / VBZ\n",
      "Print / VERB / VB\n",
      "Out / NOUN / NN\n",
      "a / DET / DT\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(words, 24):\n",
    "    print(item.text, \"/\", item.pos_, \"/\", item.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subordinating conjunction'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_past = [item.text for item in doc if item.tag_ == 'VBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Canceled', 'Killed']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(only_past, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_plural = [item.text for item in doc if item.tag_ == 'NNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: As Monkeypox Cases Keep Stacking Up ', ' Mega Millions Lottery Single Winner Snags $1.28 BILLION!!!\n",
      "\n",
      "Word: As\n",
      "Tag: IN\n",
      "Head: Keep\n",
      "Dependency relation: mark\n",
      "Children: []\n",
      "\n",
      "Word: Monkeypox\n",
      "Tag: NNP\n",
      "Head: Cases\n",
      "Dependency relation: compound\n",
      "Children: []\n",
      "\n",
      "Word: Cases\n",
      "Tag: NNS\n",
      "Head: Keep\n",
      "Dependency relation: nsubj\n",
      "Children: [Monkeypox]\n",
      "\n",
      "Word: Keep\n",
      "Tag: VBP\n",
      "Head: Keep\n",
      "Dependency relation: ROOT\n",
      "Children: [As, Cases, Stacking, ', ,, Snags, !, !, !]\n",
      "\n",
      "Word: Stacking\n",
      "Tag: VBG\n",
      "Head: Keep\n",
      "Dependency relation: xcomp\n",
      "Children: [Up]\n",
      "\n",
      "Word: Up\n",
      "Tag: RP\n",
      "Head: Stacking\n",
      "Dependency relation: prt\n",
      "Children: []\n",
      "\n",
      "Word: '\n",
      "Tag: ''\n",
      "Head: Keep\n",
      "Dependency relation: punct\n",
      "Children: []\n",
      "\n",
      "Word: ,\n",
      "Tag: ,\n",
      "Head: Keep\n",
      "Dependency relation: punct\n",
      "Children: []\n",
      "\n",
      "Word: '\n",
      "Tag: ''\n",
      "Head: Snags\n",
      "Dependency relation: punct\n",
      "Children: []\n",
      "\n",
      "Word: Mega\n",
      "Tag: NNP\n",
      "Head: Millions\n",
      "Dependency relation: compound\n",
      "Children: []\n",
      "\n",
      "Word: Millions\n",
      "Tag: NNPS\n",
      "Head: Lottery\n",
      "Dependency relation: compound\n",
      "Children: [Mega]\n",
      "\n",
      "Word: Lottery\n",
      "Tag: NNP\n",
      "Head: Winner\n",
      "Dependency relation: compound\n",
      "Children: [Millions]\n",
      "\n",
      "Word: Single\n",
      "Tag: NNP\n",
      "Head: Winner\n",
      "Dependency relation: compound\n",
      "Children: []\n",
      "\n",
      "Word: Winner\n",
      "Tag: NNP\n",
      "Head: Snags\n",
      "Dependency relation: compound\n",
      "Children: [Lottery, Single]\n",
      "\n",
      "Word: Snags\n",
      "Tag: NNS\n",
      "Head: Keep\n",
      "Dependency relation: ccomp\n",
      "Children: [', Winner, BILLION]\n",
      "\n",
      "Word: $\n",
      "Tag: $\n",
      "Head: BILLION\n",
      "Dependency relation: quantmod\n",
      "Children: []\n",
      "\n",
      "Word: 1.28\n",
      "Tag: CD\n",
      "Head: BILLION\n",
      "Dependency relation: compound\n",
      "Children: []\n",
      "\n",
      "Word: BILLION\n",
      "Tag: CD\n",
      "Head: Snags\n",
      "Dependency relation: appos\n",
      "Children: [$, 1.28]\n",
      "\n",
      "Word: !\n",
      "Tag: .\n",
      "Head: Keep\n",
      "Dependency relation: punct\n",
      "Children: []\n",
      "\n",
      "Word: !\n",
      "Tag: .\n",
      "Head: Keep\n",
      "Dependency relation: punct\n",
      "Children: []\n",
      "\n",
      "Word: !\n",
      "Tag: .\n",
      "Head: Keep\n",
      "Dependency relation: punct\n",
      "Children: []\n"
     ]
    }
   ],
   "source": [
    "sent = random.choice(sentences)\n",
    "print(\"Original sentence:\", sent.text.replace(\"\\n\", \" \"))\n",
    "for word in sent:\n",
    "    print()\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print(\"Dependency relation:\", word.dep_)\n",
    "    print(\"Children:\", list(word.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nsubj: this word's head is a verb, and this word is itself the subject of the verb\n",
    "- nsubjpass: same as above, but for subjects in sentences in the passive voice\n",
    "- dobj: this word's head is a verb, and this word is itself the direct object of the verb\n",
    "- iobj: same as above, but indirect object\n",
    "- aux: this word's head is a verb, and this word is an \"auxiliary\" verb (like \"have\", \"will\", \"be\")\n",
    "- attr: this word's head is a copula (like \"to be\"), and this is the description attributed to the subject of the sentence (e.g., in \"This product is a global brand\", brand is dependent on is with the attr dependency relation)\n",
    "- det: this word's head is a noun, and this word is a determiner of that noun (like \"the,\" \"this,\" etc.)\n",
    "- amod: this word's head is a noun, and this word is an adjective describing that noun\n",
    "- prep: this word is a preposition that modifies its head\n",
    "- pobj: this word is a dependent (object) of a preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: But I'm OK!!!\n",
      "\n",
      "Word: But\n",
      "Flattened subtree:  But\n",
      "\n",
      "Word: I\n",
      "Flattened subtree:  I\n",
      "\n",
      "Word: 'm\n",
      "Flattened subtree:  But I'm OK!!!\n",
      "\n",
      "Word: OK\n",
      "Flattened subtree:  OK\n",
      "\n",
      "Word: !\n",
      "Flattened subtree:  !\n",
      "\n",
      "Word: !\n",
      "Flattened subtree:  !\n",
      "\n",
      "Word: !\n",
      "Flattened subtree:  !\n"
     ]
    }
   ],
   "source": [
    "sent = random.choice(sentences)\n",
    "print(\"Original sentence:\", sent.text.replace(\"\\n\", \" \"))\n",
    "for word in sent:\n",
    "    print()\n",
    "    print(\"Word:\", word.text.replace(\"\\n\", \" \"))\n",
    "    print(\"Flattened subtree: \", flatten_subtree(word.subtree).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [flatten_subtree(word.subtree) for word in doc if word.dep_ in ('nsubj', 'nsubjpass')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dave Chappelle',\n",
       " 'We',\n",
       " 'Sen. Toomey',\n",
       " 'AP Bari New Maybach',\n",
       " 'This Georgia Peach',\n",
       " \"Jeff Bezos' Parents\",\n",
       " \"Beyonce 'Renaissance'\",\n",
       " 'I',\n",
       " 'What',\n",
       " 'Russia',\n",
       " 'Fight',\n",
       " 'I']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(subjects, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases = [flatten_subtree(word.subtree).replace(\"\\n\", \" \") for word in doc if word.dep_ == 'prep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "            for word in doc if word.dep_ in ('nsubj', 'nsubjpass')]\n",
    "past_tense_verbs = [word.text for word in words if word.tag_ == 'VBD' and word.lemma_ != 'be']\n",
    "adjectives = [word.text for word in words if word.tag_.startswith('JJ')]\n",
    "nouns = [word.text for word in words if word.tag_.startswith('NN')]\n",
    "prep_phrases = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "                for word in doc if word.dep_ == 'prep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracery\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I Sued After Domestic Violence Call.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = {\n",
    "    \"origin\": [\n",
    "        \"#subject.capitalize# #predicate#.\",\n",
    "        \"#subject.capitalize# #predicate#.\",\n",
    "        \"#prepphrase.capitalize#, #subject# #predicate#.\"\n",
    "    ],\n",
    "    \"predicate\": [\n",
    "        \"#verb#\",\n",
    "        \"#verb# #nounphrase#\",\n",
    "        \"#verb# #prepphrase#\"\n",
    "    ],\n",
    "    \"nounphrase\": [\n",
    "        \"the #noun#\",\n",
    "        \"the #adj# #noun#\",\n",
    "        \"the #noun# #prepphrase#\",\n",
    "        \"the #noun# and the #noun#\",\n",
    "        \"#noun.a#\",\n",
    "        \"#adj.a# #noun#\",\n",
    "        \"the #noun# that #predicate#\"\n",
    "    ],\n",
    "    \"subject\": subjects,\n",
    "    \"verb\": past_tense_verbs,\n",
    "    \"noun\": nouns,\n",
    "    \"adj\": adjectives,\n",
    "    \"prepphrase\": prep_phrases\n",
    "}\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "grammar.flatten(\"#origin#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In L.A., Sample Made. Beyoncé Wrecked a Netflix. Jon Stewart\n",
      "Got. For Gastric Sleeve, Britney Spears My Book Got a\n",
      "Beyhive. Beyoncé Got. Britney Spears My Book Killed to L.A..\n",
      "Billy Porter SCOTUS Got. You Dragged the Buster. On, True\n",
      "Bathroom Humor ', \" Michael Savarino Coach K's Grandson\n",
      "Wrecked. JAYDAYOUNGAN Sued. WITH DOMESTIC VIOLENCE, Jennifer\n",
      "Lopez Picked. WITH DOMESTIC VIOLENCE, What Got.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "output = \" \".join([grammar.flatten(\"#origin#\") for i in range(12)])\n",
    "print(fill(output, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('3.10.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "10626171d4353dd8d0f12b0dae77464b904fee8f635bb045a55f368206a04bde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
