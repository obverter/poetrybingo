{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:39.697125Z",
     "start_time": "2022-07-28T22:27:39.683710Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ntlk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000000?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstring\u001b[39;00m \u001b[39mimport\u001b[39;00m punctuation\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000000?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000000?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mntlk\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000000?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000000?line=6'>7</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ntlk'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "from spacy.lang.en import English\n",
    "from string import punctuation\n",
    "import json\n",
    "import nltk\n",
    "import os\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import shutil\n",
    "import spacy\n",
    "import sys \n",
    "import tracery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/bean/Dropbox/code/01_resources/16_models/...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('cmudict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict\u001b[0m\n\n  Searched in:\n    - '/Users/bean/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/share/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict.zip/cmudict/\u001b[0m\n\n  Searched in:\n    - '/Users/bean/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/share/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000041?line=0'>1</a>\u001b[0m cmudict \u001b[39m=\u001b[39m cmudict\u001b[39m.\u001b[39;49mdict(\u001b[39m'\u001b[39m\u001b[39m/Users/bean/Dropbox/code/01_resources/14_corpora/cmu-pronouncing/cmudict/cmudict\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mcmudict\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('cmudict')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/cmudict\u001b[0m\n\n  Searched in:\n    - '/Users/bean/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/share/nltk_data'\n    - '/Users/bean/.pyenv/versions/3.10.3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:41.756792Z",
     "start_time": "2022-07-28T22:27:40.570558Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp_en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:43.907362Z",
     "start_time": "2022-07-28T22:27:43.843959Z"
    }
   },
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('../headlines.csv')\n",
    "headline_list = headlines.headline.to_list()\n",
    "headline_str = str(headline_list)\n",
    "doc = nlp(headline_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:45.490563Z",
     "start_time": "2022-07-28T22:27:45.473425Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "words = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)\n",
    "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
    "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
    "adjs = [w for w in words if w.pos_ == \"ADJ\"]\n",
    "advs = [w for w in words if w.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:27:47.406805Z",
     "start_time": "2022-07-28T22:27:47.397338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of —\n",
      "sentences: 49\n",
      "words: 521\n",
      "noun chunks: 168\n",
      "entities: 85\n",
      "- - -\n",
      "nouns: 28\n",
      "verbs: 57\n",
      "adjs: 9\n",
      "advs: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"len of —\")\n",
    "print(f\"sentences: {len(sentences)}\")\n",
    "print(f\"words: {len(words)}\")\n",
    "print(f\"noun chunks: {len(noun_chunks)}\")\n",
    "print(f\"entities: {len(entities)}\")\n",
    "print(\"- - -\")\n",
    "print(f\"nouns: {len(nouns)}\")\n",
    "print(f\"verbs: {len(verbs)}\")\n",
    "print(f\"adjs: {len(adjs)}\")\n",
    "print(f\"advs: {len(advs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T22:28:00.162398Z",
     "start_time": "2022-07-28T22:28:00.156358Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", ' Mama June Honey Boo Boo Too Young\n",
      "\n",
      "Mike CHARGED\n",
      "\n",
      "My Car\n",
      "\n",
      "Beyoncé\n",
      "\n",
      "Love & Marriage Huntsville\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for item in random.sample(noun_chunks, 5):\n",
    "    print(item.text.strip().replace(\"\\n\", \" \"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_strs = [item.text for item in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\", \" Jim Harbaugh I\\'m Willing To Raise My Players\\' Baby ...',\n",
       " '\", \" Tony Dow \\'Leave it to Beaver\\' Star Still Alive ...',\n",
       " 'Despite Facebook Death Post\", \\' Helicopter Tragedy Man Dies Walking into Blades ...',\n",
       " \"'I'm Jamie Motherf***ing Foxx!!!'\",\n",
       " 'She Won\\'t Sit for Depo or Testify at Trial\", \\' Jerry Jones Apologizes For Derogatory Term ...',\n",
       " 'World Record \", \\' Brittney Griner I Flew To Russia Despite Travel Warning ...',\n",
       " 'If Pregnancy Unplanned\", \" \\'RHONY\\' Star Sonja Morgan Lists NYC Townhouse ...',\n",
       " 'Former Cops Get Prison Time ...',\n",
       " \"Claiming Racism ', ' Gwenyth Paltrow Hey Hailey, I f***ed your Father!!! ...\",\n",
       " 'Old Manager Costing Me $$ \", \" Jon Stewart Obliterates GOP for Blocking Vet Bill ... \\'Sen. Toomey Is a F***ing Coward\\' \", \\' Aniston, Kimmel & Bateman A-List Vacay Pals Return to L.A.!!!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(sentence_strs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sayin\n",
      "Cops\n",
      "along\n",
      "Colony\n",
      "Nanny\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(nouns, 5): # change \"nouns\" to \"verbs\" or \"adjs\" or \"advs\" to sample from those lists!\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
    "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter([w.text for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count['Jamie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 8),\n",
       " ('To', 7),\n",
       " ('For', 6),\n",
       " ('In', 5),\n",
       " ('My', 5),\n",
       " ('Star', 5),\n",
       " ('At', 4),\n",
       " ('Cops', 4),\n",
       " ('He', 4),\n",
       " ('New', 3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"words.txt\", \"w\") as fh:\n",
    "    fh.write(\"\\n\".join([w.text for w in words]))\n",
    "shutil.move(\"words.txt\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves a list of spacy values to a text file in ../data\n",
    "def save_spacy_list(filename, t):\n",
    "    with open(filename, \"w\") as fh:\n",
    "        fh.write(\"\\n\".join([item.text for item in t]))\n",
    "        ## If file exists, delete it ##\n",
    "    path = \"f'..data/{filename}'\"\n",
    "    if os.path.isfile(path):\n",
    "        os.remove(path)\n",
    "    else:## Show an error ##\n",
    "        print(f\"Error: {path} file not found\")\n",
    "    shutil.move(filename, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_values = [\n",
    "    \"noun_chunks\",\n",
    "    \"entities\",\n",
    "    \"words\",\n",
    "    \"adjs\",\n",
    "    \"advs\",\n",
    "    \"verbs\",\n",
    "    \"nouns\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: f'..data/{filename}' file not found\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Destination path '../data/words.txt' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000016?line=0'>1</a>\u001b[0m save_spacy_list(\u001b[39m\"\u001b[39;49m\u001b[39mwords.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m, words)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000016?line=1'>2</a>\u001b[0m save_spacy_list(\u001b[39m\"\u001b[39m\u001b[39mnoun_chunks.txt\u001b[39m\u001b[39m\"\u001b[39m, noun_chunks)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000016?line=2'>3</a>\u001b[0m save_spacy_list(\u001b[39m\"\u001b[39m\u001b[39mentities.txt\u001b[39m\u001b[39m\"\u001b[39m, entities)\n",
      "\u001b[1;32m/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb Cell 17\u001b[0m in \u001b[0;36msave_spacy_list\u001b[0;34m(filename, t)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000016?line=8'>9</a>\u001b[0m \u001b[39melse\u001b[39;00m:    \u001b[39m## Show an error ##\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000016?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m file not found\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m path)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/03_projects/tmz-poetry/notebooks/bard.ipynb#ch0000016?line=10'>11</a>\u001b[0m shutil\u001b[39m.\u001b[39;49mmove(filename, \u001b[39m\"\u001b[39;49m\u001b[39m../data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/shutil.py:811\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    808\u001b[0m     real_dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, _basename(src))\n\u001b[1;32m    810\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(real_dst):\n\u001b[0;32m--> 811\u001b[0m         \u001b[39mraise\u001b[39;00m Error(\u001b[39m\"\u001b[39m\u001b[39mDestination path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m already exists\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m real_dst)\n\u001b[1;32m    812\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    813\u001b[0m     os\u001b[39m.\u001b[39mrename(src, real_dst)\n",
      "\u001b[0;31mError\u001b[0m: Destination path '../data/words.txt' already exists"
     ]
    }
   ],
   "source": [
    "save_spacy_list(\"words.txt\", words)\n",
    "save_spacy_list(\"noun_chunks.txt\", noun_chunks)\n",
    "save_spacy_list(\"entities.txt\", entities)\n",
    "save_spacy_list(\"adjs.txt\", adjs)\n",
    "save_spacy_list(\"advs.txt\", advs)\n",
    "save_spacy_list(\"verbs.txt\", verbs)\n",
    "save_spacy_list(\"nouns.txt\", nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_counter_tsv(filename, counter, limit=1000):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(\"key\\tvalue\\n\")\n",
    "        for item, count in counter.most_common():\n",
    "            outfile.write(item.strip() + \"\\t\" + str(count) + \"\\n\")\n",
    "    shutil.move(filename, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_counter_tsv(\"100_common_words.tsv\", word_count, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_counter = Counter([e.text.lower() for e in people])\n",
    "save_counter_tsv(\"people_count.tsv\", people_counter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prez → Prez\n",
      "I → I\n",
      "TRO → tro\n",
      "Rights → Rights\n",
      "Says → say\n",
      "Cops → cop\n",
      "Dead → Dead\n",
      "Bari → Bari\n",
      "Car → car\n",
      "Aldrin → Aldrin\n",
      "Cover → Cover\n",
      "Lear → Lear\n"
     ]
    }
   ],
   "source": [
    "for word in random.sample(words, 12):\n",
    "    print(word.text, \"→\", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite\n",
      "Facebook\n",
      "Death\n",
      "Post\n",
      "\"\n",
      ",\n",
      "'\n",
      "Helicopter\n",
      "Tragedy\n",
      "Man\n",
      "Dies\n",
      "Walking\n",
      "into\n",
      "Blades\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sentence = random.choice(sentences)\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I / PRON / PRP\n",
      "Zelensky / PROPN / NNP\n",
      "Jerry / PROPN / NNP\n",
      "Tony / PROPN / NNP\n",
      "OK / ADJ / JJ\n",
      "SHEREE / PROPN / NNP\n",
      "Renaissance / PROPN / NNP\n",
      "My / PRON / PRP$\n",
      "Pregnancy / PROPN / NNP\n",
      "it / PRON / PRP\n",
      "Trump / PROPN / NNP\n",
      "NFLer / PROPN / NNP\n",
      "Dow / PROPN / NNP\n",
      "Buy / VERB / VBP\n",
      "Maybach / PROPN / NNP\n",
      "at / ADP / IN\n",
      "Despite / SCONJ / IN\n",
      "I / PRON / PRP\n",
      "Again / ADV / RB\n",
      "Throws / VERB / VBZ\n",
      "Involving / VERB / VBG\n",
      "Kanye / PROPN / NNP\n",
      "If / SCONJ / IN\n",
      "Record / PROPN / NNP\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(words, 24):\n",
    "    print(item.text, \"/\", item.pos_, \"/\", item.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subordinating conjunction'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_past = [item.text for item in doc if item.tag_ == 'VBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deposed', 'Alleged']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(only_past, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_plural = [item.text for item in doc if item.tag_ == 'NNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = random.choice(sentences)\n",
    "print(\"Original sentence:\", sent.text.replace(\"\\n\", \" \"))\n",
    "for word in sent:\n",
    "    print()\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print(\"Dependency relation:\", word.dep_)\n",
    "    print(\"Children:\", list(word.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nsubj: this word's head is a verb, and this word is itself the subject of the verb\n",
    "- nsubjpass: same as above, but for subjects in sentences in the passive voice\n",
    "- dobj: this word's head is a verb, and this word is itself the direct object of the verb\n",
    "- iobj: same as above, but indirect object\n",
    "- aux: this word's head is a verb, and this word is an \"auxiliary\" verb (like \"have\", \"will\", \"be\")\n",
    "- attr: this word's head is a copula (like \"to be\"), and this is the description attributed to the subject of the sentence (e.g., in \"This product is a global brand\", brand is dependent on is with the attr dependency relation)\n",
    "- det: this word's head is a noun, and this word is a determiner of that noun (like \"the,\" \"this,\" etc.)\n",
    "- amod: this word's head is a noun, and this word is an adjective describing that noun\n",
    "- prep: this word is a preposition that modifies its head\n",
    "- pobj: this word is a dependent (object) of a preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = random.choice(sentences)\n",
    "print(\"Original sentence:\", sent.text.replace(\"\\n\", \" \"))\n",
    "for word in sent:\n",
    "    print()\n",
    "    print(\"Word:\", word.text.replace(\"\\n\", \" \"))\n",
    "    print(\"Flattened subtree: \", flatten_subtree(word.subtree).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [flatten_subtree(word.subtree) for word in doc if word.dep_ in ('nsubj', 'nsubjpass')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PetSmart',\n",
       " 'Tony Dow',\n",
       " 'Brittney Griner',\n",
       " 'Britney Spears',\n",
       " \"'s\",\n",
       " 'I',\n",
       " 'Cops',\n",
       " 'Helicopter Tragedy Man',\n",
       " 'He',\n",
       " 'Your Groomers',\n",
       " 'Former Cops',\n",
       " \"ITo Raise My Players' Baby\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(subjects, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases = [flatten_subtree(word.subtree).replace(\"\\n\", \" \") for word in doc if word.dep_ == 'prep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "            for word in doc if word.dep_ in ('nsubj', 'nsubjpass')]\n",
    "past_tense_verbs = [word.text for word in words if word.tag_ == 'VBD' and word.lemma_ != 'be']\n",
    "adjectives = [word.text for word in words if word.tag_.startswith('JJ')]\n",
    "nouns = [word.text for word in words if word.tag_.startswith('NN')]\n",
    "prep_phrases = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "                for word in doc if word.dep_ == 'prep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracery\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tony Dow Sued the Rey.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = {\n",
    "    \"origin\": [\n",
    "        \"#subject.capitalize# #predicate#.\",\n",
    "        \"#subject.capitalize# #predicate#.\",\n",
    "        \"#prepphrase.capitalize#, #subject# #predicate#.\"\n",
    "    ],\n",
    "    \"predicate\": [\n",
    "        \"#verb#\",\n",
    "        \"#verb# #nounphrase#\",\n",
    "        \"#verb# #prepphrase#\"\n",
    "    ],\n",
    "    \"nounphrase\": [\n",
    "        \"the #noun#\",\n",
    "        \"the #adj# #noun#\",\n",
    "        \"the #noun# #prepphrase#\",\n",
    "        \"the #noun# and the #noun#\",\n",
    "        \"#noun.a#\",\n",
    "        \"#adj.a# #noun#\",\n",
    "        \"the #noun# that #predicate#\"\n",
    "    ],\n",
    "    \"subject\": subjects,\n",
    "    \"verb\": past_tense_verbs,\n",
    "    \"noun\": nouns,\n",
    "    \"adj\": adjectives,\n",
    "    \"prepphrase\": prep_phrases\n",
    "}\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "grammar.flatten(\"#origin#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Dow Wrecked. Brittney Griner Wrecked the Former Morgan.\n",
      "For Derogatory Term, Brittney Griner Killed a Cops. For\n",
      "Violating His Civil Rights \", ' Kanye West Gifts A$AP Bari\n",
      "New Maybach… Days After He Wrecked His Old One!!! ', \"\n",
      "Norman Lear Singin', Jeff Bezos Parents Wrecked the Break in\n",
      "FL. To Russia, Cops Unplanned the Hart and the Players. Your\n",
      "Groomers Wrecked the RHONY that Stole In Five Minutes. I\n",
      "Stole Since Jan. 6', \" Kevin Hart Shames Ex-NFLer Over\n",
      "Nudist Colony Hobby With Wife. Motherf***ing Foxx, Your\n",
      "Groomers Dragged. He Sued the Honey at Trial. I Stole an OK\n",
      "Exam. At 100, He Killed. At Customers, Helicopter Tragedy\n",
      "Man Killed WITH DOMESTIC VIOLENCE.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "output = \" \".join([grammar.flatten(\"#origin#\") for i in range(12)])\n",
    "print(fill(output, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('3.10.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "10626171d4353dd8d0f12b0dae77464b904fee8f635bb045a55f368206a04bde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
