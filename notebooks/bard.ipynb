{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import tracery\n",
    "import os\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "nlp_en = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = pd.read_csv('../headlines.csv')\n",
    "headline_list = headlines.headline.to_list()\n",
    "headline_str = str(headline_list)\n",
    "doc = nlp(headline_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(doc.sents)\n",
    "words = [w for w in list(doc) if w.is_alpha]\n",
    "noun_chunks = list(doc.noun_chunks)\n",
    "entities = list(doc.ents)\n",
    "nouns = [w for w in words if w.pos_ == \"NOUN\"]\n",
    "verbs = [w for w in words if w.pos_ == \"VERB\"]\n",
    "adjs = [w for w in words if w.pos_ == \"ADJ\"]\n",
    "advs = [w for w in words if w.pos_ == \"ADV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of —\n",
      "sentences: 32\n",
      "words: 336\n",
      "noun chunks: 111\n",
      "entities: 53\n",
      "- - -\n",
      "nouns: 17\n",
      "verbs: 37\n",
      "adjs: 4\n",
      "advs: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"len of —\")\n",
    "print(f\"sentences: {len(sentences)}\")\n",
    "print(f\"words: {len(words)}\")\n",
    "print(f\"noun chunks: {len(noun_chunks)}\")\n",
    "print(f\"entities: {len(entities)}\")\n",
    "print(\"- - -\")\n",
    "print(f\"nouns: {len(nouns)}\")\n",
    "print(f\"verbs: {len(verbs)}\")\n",
    "print(f\"adjs: {len(adjs)}\")\n",
    "print(f\"advs: {len(advs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gastric Sleeve\n",
      "\n",
      "California Soccer Player\n",
      "\n",
      "Depo\n",
      "\n",
      "Beaver' Star Dead\n",
      "\n",
      "RHONY' Star Sonja Morgan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "for item in random.sample(noun_chunks, 5):\n",
    "    print(item.text.strip().replace(\"\\n\", \" \"))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_strs = [item.text for item in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If Pregnancy Unplanned\", \" \\'RHONY\\' Star Sonja Morgan Lists NYC Townhouse ...',\n",
       " \"But I'm OK!!!\",\n",
       " \"Involving Players & Fans', ' Lana Del Rey Gets TRO Against Alleged Stalker ...\",\n",
       " \"For Mental Health Break ', ' Mama June Honey Boo Boo Too Young For Gastric Sleeve!!!\",\n",
       " '\\', \" \\'RHOA\\' STAR SHEREE WHITFIELD',\n",
       " 'For 1st Speech Back in D.C.',\n",
       " 'Since Jan. 6\\', \" Kevin Hart Shames Ex-NFLer Over Nudist Colony Hobby With Wife ... \\'You Hear Whatchu Sayin\\'!?!\\'',\n",
       " \"'I'm Jamie Motherf***ing Foxx!!!'\",\n",
       " \"At Cowboys Camp ', ' Buzz Aldrin Apollo 11 Jacket Sells For $2.7 Mil ... Space Artifact Record!!!', ' Donald Trump Protesters Flood Hotel ...\",\n",
       " '\", \" Jim Harbaugh I\\'m Willing To Raise My Players\\' Baby ...']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(sentence_strs, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message\n",
      "Shoot\n",
      "Cops\n",
      "Customers\n",
      "Sayin\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(nouns, 5): # change \"nouns\" to \"verbs\" or \"adjs\" or \"advs\" to sample from those lists!\n",
    "    print(item.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = [e for e in entities if e.label_ == \"PERSON\"]\n",
    "locations = [e for e in entities if e.label_ == \"LOC\"]\n",
    "times = [e for e in entities if e.label_ == \"TIME\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_count = Counter([w.text for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count['Jamie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('For', 6),\n",
       " ('To', 5),\n",
       " ('I', 5),\n",
       " ('My', 4),\n",
       " ('Star', 4),\n",
       " ('In', 3),\n",
       " ('At', 3),\n",
       " ('Wife', 3),\n",
       " ('He', 3),\n",
       " ('at', 3)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"words.txt\", \"w\") as fh:\n",
    "    fh.write(\"\\n\".join([w.text for w in words]))\n",
    "shutil.move(\"words.txt\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves a list of spacy values to a text file in ../data\n",
    "def save_spacy_list(filename, t):\n",
    "    with open(filename, \"w\") as fh:\n",
    "        fh.write(\"\\n\".join([item.text for item in t]))\n",
    "    shutil.move(filename, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_values = [\n",
    "    \"noun_chunks\",\n",
    "    \"entities\",\n",
    "    \"words\",\n",
    "    \"adjs\",\n",
    "    \"advs\",\n",
    "    \"verbs\",\n",
    "    \"nouns\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_spacy_list(\"words.txt\", words)\n",
    "save_spacy_list(\"noun_chunks.txt\", noun_chunks)\n",
    "save_spacy_list(\"entities.txt\", entities)\n",
    "save_spacy_list(\"adjs.txt\", adjs)\n",
    "save_spacy_list(\"advs.txt\", advs)\n",
    "save_spacy_list(\"verbs.txt\", verbs)\n",
    "save_spacy_list(\"nouns.txt\", nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_counter_tsv(filename, counter, limit=1000):\n",
    "    with open(filename, \"w\") as outfile:\n",
    "        outfile.write(\"key\\tvalue\\n\")\n",
    "        for item, count in counter.most_common():\n",
    "            outfile.write(item.strip() + \"\\t\" + str(count) + \"\\n\")\n",
    "    shutil.move(filename, \"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_counter_tsv(\"100_common_words.tsv\", word_count, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_counter = Counter([e.text.lower() for e in people])\n",
    "save_counter_tsv(\"people_count.tsv\", people_counter, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prez → Prez\n",
      "I → I\n",
      "TRO → tro\n",
      "Rights → Rights\n",
      "Says → say\n",
      "Cops → cop\n",
      "Dead → Dead\n",
      "Bari → Bari\n",
      "Car → car\n",
      "Aldrin → Aldrin\n",
      "Cover → Cover\n",
      "Lear → Lear\n"
     ]
    }
   ],
   "source": [
    "for word in random.sample(words, 12):\n",
    "    print(word.text, \"→\", word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Despite\n",
      "Facebook\n",
      "Death\n",
      "Post\n",
      "\"\n",
      ",\n",
      "'\n",
      "Helicopter\n",
      "Tragedy\n",
      "Man\n",
      "Dies\n",
      "Walking\n",
      "into\n",
      "Blades\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "sentence = random.choice(sentences)\n",
    "for word in sentence:\n",
    "    print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I / PRON / PRP\n",
      "Zelensky / PROPN / NNP\n",
      "Jerry / PROPN / NNP\n",
      "Tony / PROPN / NNP\n",
      "OK / ADJ / JJ\n",
      "SHEREE / PROPN / NNP\n",
      "Renaissance / PROPN / NNP\n",
      "My / PRON / PRP$\n",
      "Pregnancy / PROPN / NNP\n",
      "it / PRON / PRP\n",
      "Trump / PROPN / NNP\n",
      "NFLer / PROPN / NNP\n",
      "Dow / PROPN / NNP\n",
      "Buy / VERB / VBP\n",
      "Maybach / PROPN / NNP\n",
      "at / ADP / IN\n",
      "Despite / SCONJ / IN\n",
      "I / PRON / PRP\n",
      "Again / ADV / RB\n",
      "Throws / VERB / VBZ\n",
      "Involving / VERB / VBG\n",
      "Kanye / PROPN / NNP\n",
      "If / SCONJ / IN\n",
      "Record / PROPN / NNP\n"
     ]
    }
   ],
   "source": [
    "for item in random.sample(words, 24):\n",
    "    print(item.text, \"/\", item.pos_, \"/\", item.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'subordinating conjunction'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('SCONJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_past = [item.text for item in doc if item.tag_ == 'VBN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deposed', 'Alleged']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(only_past, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_plural = [item.text for item in doc if item.tag_ == 'NNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = random.choice(sentences)\n",
    "print(\"Original sentence:\", sent.text.replace(\"\\n\", \" \"))\n",
    "for word in sent:\n",
    "    print()\n",
    "    print(\"Word:\", word.text)\n",
    "    print(\"Tag:\", word.tag_)\n",
    "    print(\"Head:\", word.head.text)\n",
    "    print(\"Dependency relation:\", word.dep_)\n",
    "    print(\"Children:\", list(word.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nsubj: this word's head is a verb, and this word is itself the subject of the verb\n",
    "- nsubjpass: same as above, but for subjects in sentences in the passive voice\n",
    "- dobj: this word's head is a verb, and this word is itself the direct object of the verb\n",
    "- iobj: same as above, but indirect object\n",
    "- aux: this word's head is a verb, and this word is an \"auxiliary\" verb (like \"have\", \"will\", \"be\")\n",
    "- attr: this word's head is a copula (like \"to be\"), and this is the description attributed to the subject of the sentence (e.g., in \"This product is a global brand\", brand is dependent on is with the attr dependency relation)\n",
    "- det: this word's head is a noun, and this word is a determiner of that noun (like \"the,\" \"this,\" etc.)\n",
    "- amod: this word's head is a noun, and this word is an adjective describing that noun\n",
    "- prep: this word is a preposition that modifies its head\n",
    "- pobj: this word is a dependent (object) of a preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_subtree(st):\n",
    "    return ''.join([w.text_with_ws for w in list(st)]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = random.choice(sentences)\n",
    "print(\"Original sentence:\", sent.text.replace(\"\\n\", \" \"))\n",
    "for word in sent:\n",
    "    print()\n",
    "    print(\"Word:\", word.text.replace(\"\\n\", \" \"))\n",
    "    print(\"Flattened subtree: \", flatten_subtree(word.subtree).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = []\n",
    "for word in doc:\n",
    "    if word.dep_ in ('nsubj', 'nsubjpass'):\n",
    "        subjects.append(flatten_subtree(word.subtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PetSmart',\n",
       " 'Tony Dow',\n",
       " 'Brittney Griner',\n",
       " 'Britney Spears',\n",
       " \"'s\",\n",
       " 'I',\n",
       " 'Cops',\n",
       " 'Helicopter Tragedy Man',\n",
       " 'He',\n",
       " 'Your Groomers',\n",
       " 'Former Cops',\n",
       " \"ITo Raise My Players' Baby\"]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(subjects, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_phrases = []\n",
    "for word in doc:\n",
    "    if word.dep_ == 'prep':\n",
    "        prep_phrases.append(flatten_subtree(word.subtree).replace(\"\\n\", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "            for word in doc if word.dep_ in ('nsubj', 'nsubjpass')]\n",
    "past_tense_verbs = [word.text for word in words if word.tag_ == 'VBD' and word.lemma_ != 'be']\n",
    "adjectives = [word.text for word in words if word.tag_.startswith('JJ')]\n",
    "nouns = [word.text for word in words if word.tag_.startswith('NN')]\n",
    "prep_phrases = [flatten_subtree(word.subtree).replace(\"\\n\", \" \")\n",
    "                for word in doc if word.dep_ == 'prep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tracery\n",
    "from tracery.modifiers import base_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tony Dow Sued the Rey.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules = {\n",
    "    \"origin\": [\n",
    "        \"#subject.capitalize# #predicate#.\",\n",
    "        \"#subject.capitalize# #predicate#.\",\n",
    "        \"#prepphrase.capitalize#, #subject# #predicate#.\"\n",
    "    ],\n",
    "    \"predicate\": [\n",
    "        \"#verb#\",\n",
    "        \"#verb# #nounphrase#\",\n",
    "        \"#verb# #prepphrase#\"\n",
    "    ],\n",
    "    \"nounphrase\": [\n",
    "        \"the #noun#\",\n",
    "        \"the #adj# #noun#\",\n",
    "        \"the #noun# #prepphrase#\",\n",
    "        \"the #noun# and the #noun#\",\n",
    "        \"#noun.a#\",\n",
    "        \"#adj.a# #noun#\",\n",
    "        \"the #noun# that #predicate#\"\n",
    "    ],\n",
    "    \"subject\": subjects,\n",
    "    \"verb\": past_tense_verbs,\n",
    "    \"noun\": nouns,\n",
    "    \"adj\": adjectives,\n",
    "    \"prepphrase\": prep_phrases\n",
    "}\n",
    "grammar = tracery.Grammar(rules)\n",
    "grammar.add_modifiers(base_english)\n",
    "grammar.flatten(\"#origin#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony Dow Wrecked. Brittney Griner Wrecked the Former Morgan.\n",
      "For Derogatory Term, Brittney Griner Killed a Cops. For\n",
      "Violating His Civil Rights \", ' Kanye West Gifts A$AP Bari\n",
      "New Maybach… Days After He Wrecked His Old One!!! ', \"\n",
      "Norman Lear Singin', Jeff Bezos Parents Wrecked the Break in\n",
      "FL. To Russia, Cops Unplanned the Hart and the Players. Your\n",
      "Groomers Wrecked the RHONY that Stole In Five Minutes. I\n",
      "Stole Since Jan. 6', \" Kevin Hart Shames Ex-NFLer Over\n",
      "Nudist Colony Hobby With Wife. Motherf***ing Foxx, Your\n",
      "Groomers Dragged. He Sued the Honey at Trial. I Stole an OK\n",
      "Exam. At 100, He Killed. At Customers, Helicopter Tragedy\n",
      "Man Killed WITH DOMESTIC VIOLENCE.\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "output = \" \".join([grammar.flatten(\"#origin#\") for i in range(12)])\n",
    "print(fill(output, 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('3.10.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10626171d4353dd8d0f12b0dae77464b904fee8f635bb045a55f368206a04bde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
