{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from imagen_pytorch import Unet, Imagen, SRUnet256, ImagenTrainer, ElucidatedImagen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your unets ...\n",
    "\n",
    "imagen = ElucidatedImagen(\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 128),\n",
    "    cond_drop_prob = 0.1,\n",
    "    num_sample_steps = (64, 32), # number of sample steps - 64 for base unet, 32 for upsampler (just an example, have no clue what the optimal values are)\n",
    "    sigma_min = 0.002,           # min noise level\n",
    "    sigma_max = (80, 160),       # max noise level, @crowsonkb recommends double the max noise level for upsampler\n",
    "    sigma_data = 0.5,            # standard deviation of data distribution\n",
    "    rho = 7,                     # controls the sampling schedule\n",
    "    P_mean = -1.2,               # mean of log-normal distribution from which noise is drawn for training\n",
    "    P_std = 1.2,                 # standard deviation of log-normal distribution from which noise is drawn for training\n",
    "    S_churn = 80,                # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n",
    "    S_tmin = 0.05,\n",
    "    S_tmax = 50,\n",
    "    S_noise = 1.003,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192ae15dd018477190661b91e8d14359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/amp/autocast_mode.py:198: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn('User provided device_type of \\'cuda\\', but CUDA is not available. Disabling')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc648a41b75466db3a3cf32a305dc71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/2.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569e94eea53a47d082c1f1752853c9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bean/.pyenv/versions/3.10.3/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "sampling loop time step: 100%|██████████| 1000/1000 [07:41<00:00,  2.17it/s]\n",
      "sampling loop time step:  25%|██▍       | 247/1000 [14:54<45:25,  3.62s/it]\n",
      "1it [22:35, 1355.38s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=47'>48</a>\u001b[0m trainer\u001b[39m.\u001b[39mupdate(unet_number \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=49'>50</a>\u001b[0m \u001b[39m# do the above for many many many many steps\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=50'>51</a>\u001b[0m \u001b[39m# now you can sample an image based on the text embeddings from the cascading ddpm\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=52'>53</a>\u001b[0m images \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49msample(texts \u001b[39m=\u001b[39;49m [\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=53'>54</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39ma puppy looking anxiously at a giant donut on the table\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=54'>55</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mthe milky way galaxy in the style of monet\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=55'>56</a>\u001b[0m ], cond_scale \u001b[39m=\u001b[39;49m \u001b[39m3.\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000014?line=57'>58</a>\u001b[0m images\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/trainer.py:130\u001b[0m, in \u001b[0;36mcast_torch_tensor.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m args, kwargs_values \u001b[39m=\u001b[39m all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n\u001b[1;32m    128\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(kwargs_keys, kwargs_values)))\n\u001b[0;32m--> 130\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/trainer.py:190\u001b[0m, in \u001b[0;36mimagen_sample_in_chunks.<locals>.inner\u001b[0;34m(self, max_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_batch_size \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(max_batch_size):\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimagen\u001b[39m.\u001b[39munconditional:\n\u001b[1;32m    193\u001b[0m         batch_size \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/trainer.py:884\u001b[0m, in \u001b[0;36mImagenTrainer.sample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_untrained_unets()        \n\u001b[1;32m    883\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m--> 884\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimagen\u001b[39m.\u001b[39;49msample(\u001b[39m*\u001b[39;49margs, device \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, use_tqdm \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_main, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:93\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m was_training \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining\n\u001b[1;32m     92\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 93\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     94\u001b[0m model\u001b[39m.\u001b[39mtrain(was_training)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:2134\u001b[0m, in \u001b[0;36mImagen.sample\u001b[0;34m(self, texts, text_masks, text_embeds, cond_images, inpaint_images, inpaint_masks, inpaint_resample_times, batch_size, cond_scale, lowres_sample_noise_level, stop_at_unet_number, return_all_unet_outputs, return_pil_images, device, use_tqdm)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         lowres_cond_img, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlowres_noise_schedule\u001b[39m.\u001b[39mq_sample(x_start \u001b[39m=\u001b[39m lowres_cond_img, t \u001b[39m=\u001b[39m lowres_noise_times, noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(lowres_cond_img))\n\u001b[1;32m   2132\u001b[0m     shape \u001b[39m=\u001b[39m (batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels, image_size, image_size)\n\u001b[0;32m-> 2134\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_loop(\n\u001b[1;32m   2135\u001b[0m         unet,\n\u001b[1;32m   2136\u001b[0m         shape,\n\u001b[1;32m   2137\u001b[0m         text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2138\u001b[0m         text_mask \u001b[39m=\u001b[39;49m text_masks,\n\u001b[1;32m   2139\u001b[0m         cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2140\u001b[0m         inpaint_images \u001b[39m=\u001b[39;49m inpaint_images,\n\u001b[1;32m   2141\u001b[0m         inpaint_masks \u001b[39m=\u001b[39;49m inpaint_masks,\n\u001b[1;32m   2142\u001b[0m         inpaint_resample_times \u001b[39m=\u001b[39;49m inpaint_resample_times,\n\u001b[1;32m   2143\u001b[0m         cond_scale \u001b[39m=\u001b[39;49m unet_cond_scale,\n\u001b[1;32m   2144\u001b[0m         lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2145\u001b[0m         lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2146\u001b[0m         noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2147\u001b[0m         pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2148\u001b[0m         dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold,\n\u001b[1;32m   2149\u001b[0m         use_tqdm \u001b[39m=\u001b[39;49m use_tqdm\n\u001b[1;32m   2150\u001b[0m     )\n\u001b[1;32m   2152\u001b[0m     outputs\u001b[39m.\u001b[39mappend(img)\n\u001b[1;32m   2154\u001b[0m \u001b[39mif\u001b[39;00m exists(stop_at_unet_number) \u001b[39mand\u001b[39;00m stop_at_unet_number \u001b[39m==\u001b[39m unet_number:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:2030\u001b[0m, in \u001b[0;36mImagen.p_sample_loop\u001b[0;34m(self, unet, shape, noise_scheduler, lowres_cond_img, lowres_noise_times, text_embeds, text_mask, cond_images, inpaint_images, inpaint_masks, inpaint_resample_times, cond_scale, pred_objective, dynamic_threshold, use_tqdm)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     noised_inpaint_images, _ \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mq_sample(inpaint_images, t \u001b[39m=\u001b[39m times)\n\u001b[1;32m   2028\u001b[0m     img \u001b[39m=\u001b[39m img \u001b[39m*\u001b[39m \u001b[39m~\u001b[39minpaint_masks \u001b[39m+\u001b[39m noised_inpaint_images \u001b[39m*\u001b[39m inpaint_masks\n\u001b[0;32m-> 2030\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(\n\u001b[1;32m   2031\u001b[0m     unet,\n\u001b[1;32m   2032\u001b[0m     img,\n\u001b[1;32m   2033\u001b[0m     times,\n\u001b[1;32m   2034\u001b[0m     t_next \u001b[39m=\u001b[39;49m times_next,\n\u001b[1;32m   2035\u001b[0m     text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2036\u001b[0m     text_mask \u001b[39m=\u001b[39;49m text_mask,\n\u001b[1;32m   2037\u001b[0m     cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2038\u001b[0m     cond_scale \u001b[39m=\u001b[39;49m cond_scale,\n\u001b[1;32m   2039\u001b[0m     lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2040\u001b[0m     lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2041\u001b[0m     noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2042\u001b[0m     pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2043\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold\n\u001b[1;32m   2044\u001b[0m )\n\u001b[1;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m has_inpainting \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_last_resample_step \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39mall(is_last_timestep)):\n\u001b[1;32m   2047\u001b[0m     renoised_img \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mq_sample_from_to(img, times_next, times)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1973\u001b[0m, in \u001b[0;36mImagen.p_sample\u001b[0;34m(self, unet, x, t, noise_scheduler, t_next, text_embeds, text_mask, cond_images, cond_scale, lowres_cond_img, lowres_noise_times, pred_objective, dynamic_threshold)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m   1955\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_sample\u001b[39m(\n\u001b[1;32m   1956\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m ):\n\u001b[1;32m   1972\u001b[0m     b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m-> 1973\u001b[0m     model_mean, _, model_log_variance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(unet, x \u001b[39m=\u001b[39;49m x, t \u001b[39m=\u001b[39;49m t, t_next \u001b[39m=\u001b[39;49m t_next, noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler, text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_mask, cond_images \u001b[39m=\u001b[39;49m cond_images, cond_scale \u001b[39m=\u001b[39;49m cond_scale, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times, pred_objective \u001b[39m=\u001b[39;49m pred_objective, dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold)\n\u001b[1;32m   1974\u001b[0m     noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x)\n\u001b[1;32m   1975\u001b[0m     \u001b[39m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1928\u001b[0m, in \u001b[0;36mImagen.p_mean_variance\u001b[0;34m(self, unet, x, t, noise_scheduler, text_embeds, text_mask, cond_images, lowres_cond_img, lowres_noise_times, cond_scale, model_output, t_next, pred_objective, dynamic_threshold)\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\n\u001b[1;32m   1909\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1910\u001b[0m     unet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m ):\n\u001b[1;32m   1926\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (cond_scale \u001b[39m!=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_classifier_guidance), \u001b[39m'\u001b[39m\u001b[39mimagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1928\u001b[0m     pred \u001b[39m=\u001b[39m default(model_output, \u001b[39mlambda\u001b[39;49;00m: unet\u001b[39m.\u001b[39;49mforward_with_cond_scale(x, noise_scheduler\u001b[39m.\u001b[39;49mget_condition(t), text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_mask, cond_images \u001b[39m=\u001b[39;49m cond_images, cond_scale \u001b[39m=\u001b[39;49m cond_scale, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_noise_times \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowres_noise_schedule\u001b[39m.\u001b[39;49mget_condition(lowres_noise_times)))\n\u001b[1;32m   1930\u001b[0m     \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1931\u001b[0m         x_start \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mpredict_start_from_noise(x, t \u001b[39m=\u001b[39m t, noise \u001b[39m=\u001b[39m pred)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:63\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(val, d)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m exists(val):\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m val\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m d() \u001b[39mif\u001b[39;00m callable(d) \u001b[39melse\u001b[39;00m d\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1928\u001b[0m, in \u001b[0;36mImagen.p_mean_variance.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\n\u001b[1;32m   1909\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1910\u001b[0m     unet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m ):\n\u001b[1;32m   1926\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (cond_scale \u001b[39m!=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_classifier_guidance), \u001b[39m'\u001b[39m\u001b[39mimagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1928\u001b[0m     pred \u001b[39m=\u001b[39m default(model_output, \u001b[39mlambda\u001b[39;00m: unet\u001b[39m.\u001b[39;49mforward_with_cond_scale(x, noise_scheduler\u001b[39m.\u001b[39;49mget_condition(t), text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_mask, cond_images \u001b[39m=\u001b[39;49m cond_images, cond_scale \u001b[39m=\u001b[39;49m cond_scale, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_noise_times \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowres_noise_schedule\u001b[39m.\u001b[39;49mget_condition(lowres_noise_times)))\n\u001b[1;32m   1930\u001b[0m     \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1931\u001b[0m         x_start \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mpredict_start_from_noise(x, t \u001b[39m=\u001b[39m t, noise \u001b[39m=\u001b[39m pred)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1443\u001b[0m, in \u001b[0;36mUnet.forward_with_cond_scale\u001b[0;34m(self, cond_scale, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[39mif\u001b[39;00m cond_scale \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1441\u001b[0m     \u001b[39mreturn\u001b[39;00m logits\n\u001b[0;32m-> 1443\u001b[0m null_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, cond_drop_prob \u001b[39m=\u001b[39;49m \u001b[39m1.\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m \u001b[39mreturn\u001b[39;00m null_logits \u001b[39m+\u001b[39m (logits \u001b[39m-\u001b[39m null_logits) \u001b[39m*\u001b[39m cond_scale\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1615\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, lowres_cond_img, lowres_noise_times, text_embeds, text_mask, cond_images, cond_drop_prob)\u001b[0m\n\u001b[1;32m   1613\u001b[0m \u001b[39mfor\u001b[39;00m init_block, resnet_blocks, attn_block, upsample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mups:\n\u001b[1;32m   1614\u001b[0m     x \u001b[39m=\u001b[39m add_skip_connection(x)\n\u001b[0;32m-> 1615\u001b[0m     x \u001b[39m=\u001b[39m init_block(x, t, c)\n\u001b[1;32m   1617\u001b[0m     \u001b[39mfor\u001b[39;00m resnet_block \u001b[39min\u001b[39;00m resnet_blocks:\n\u001b[1;32m   1618\u001b[0m         x \u001b[39m=\u001b[39m add_skip_connection(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:695\u001b[0m, in \u001b[0;36mResnetBlock.forward\u001b[0;34m(self, x, time_emb, cond)\u001b[0m\n\u001b[1;32m    692\u001b[0m     time_emb \u001b[39m=\u001b[39m rearrange(time_emb, \u001b[39m'\u001b[39m\u001b[39mb c -> b c 1 1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    693\u001b[0m     scale_shift \u001b[39m=\u001b[39m time_emb\u001b[39m.\u001b[39mchunk(\u001b[39m2\u001b[39m, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m--> 695\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock1(x)\n\u001b[1;32m    697\u001b[0m \u001b[39mif\u001b[39;00m exists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcross_attn):\n\u001b[1;32m    698\u001b[0m     \u001b[39massert\u001b[39;00m exists(cond)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:638\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, scale_shift)\u001b[0m\n\u001b[1;32m    635\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m (scale \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m shift\n\u001b[1;32m    637\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[0;32m--> 638\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproject(x)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# unet for imagen\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True, True),\n",
    ")\n",
    "\n",
    "unet2 = Unet(\n",
    "    dim = 32,\n",
    "    cond_dim = 512,\n",
    "    dim_mults = (1, 2, 4, 8),\n",
    "    num_resnet_blocks = (2, 4, 8, 8),\n",
    "    layer_attns = (False, False, False, True),\n",
    "    layer_cross_attns = (False, False, False, True)\n",
    ")\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    unets = (unet1, unet2),\n",
    "    text_encoder_name = 't5-large',\n",
    "    image_sizes = (64, 256),\n",
    "    timesteps = 1000,\n",
    "    cond_drop_prob = 0.1\n",
    ")\n",
    "\n",
    "# wrap imagen with the trainer class\n",
    "\n",
    "trainer = ImagenTrainer(imagen)\n",
    "\n",
    "# mock images (get a lot of this) and text encodings from large T5\n",
    "\n",
    "text_embeds = torch.randn(64, 256, 1024)\n",
    "images = torch.randn(64, 3, 256, 256)\n",
    "\n",
    "# feed images into imagen, training each unet in the cascade\n",
    "\n",
    "loss = trainer(\n",
    "    images,\n",
    "    text_embeds = text_embeds,\n",
    "    unet_number = 1,            # training on unet number 1 in this example, but you will have to also save checkpoints and then reload and continue training on unet number 2\n",
    "    max_batch_size = 4          # auto divide the batch of 64 up into batch size of 4 and accumulate gradients, so it all fits in memory\n",
    ")\n",
    "\n",
    "trainer.update(unet_number = 1)\n",
    "\n",
    "# do the above for many many many many steps\n",
    "# now you can sample an image based on the text embeddings from the cascading ddpm\n",
    "\n",
    "images = trainer.sample(texts = [\n",
    "    'a puppy looking anxiously at a giant donut on the table',\n",
    "    'the milky way galaxy in the style of monet'\n",
    "], cond_scale = 3.)\n",
    "\n",
    "images.shape # (2, 3, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unet 2 has not been trained\n",
      "when sampling, you can pass stop_at_unet_number to stop early in the cascade, so it does not try to generate with untrained unets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step:   2%|▏         | 24/1000 [01:15<50:53,  3.13s/it]\n",
      "0it [01:15, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000012?line=38'>39</a>\u001b[0m trainer\u001b[39m.\u001b[39mupdate(unet_number \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000012?line=40'>41</a>\u001b[0m \u001b[39m# do the above for many many many many steps\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000012?line=41'>42</a>\u001b[0m \u001b[39m# now you can sample images unconditionally from the cascading unet(s)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/bean/Dropbox/code/_projects/tmz-poetry/notebooks/painter.ipynb#ch0000012?line=43'>44</a>\u001b[0m images \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49msample(batch_size \u001b[39m=\u001b[39;49m \u001b[39m16\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/trainer.py:130\u001b[0m, in \u001b[0;36mcast_torch_tensor.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m args, kwargs_values \u001b[39m=\u001b[39m all_args[:split_kwargs_index], all_args[split_kwargs_index:]\n\u001b[1;32m    128\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mtuple\u001b[39m(\u001b[39mzip\u001b[39m(kwargs_keys, kwargs_values)))\n\u001b[0;32m--> 130\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/trainer.py:190\u001b[0m, in \u001b[0;36mimagen_sample_in_chunks.<locals>.inner\u001b[0;34m(self, max_batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m@wraps\u001b[39m(fn)\n\u001b[1;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_batch_size \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    189\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists(max_batch_size):\n\u001b[0;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    192\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimagen\u001b[39m.\u001b[39munconditional:\n\u001b[1;32m    193\u001b[0m         batch_size \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/trainer.py:884\u001b[0m, in \u001b[0;36mImagenTrainer.sample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    881\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_untrained_unets()        \n\u001b[1;32m    883\u001b[0m \u001b[39mwith\u001b[39;00m context():\n\u001b[0;32m--> 884\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimagen\u001b[39m.\u001b[39;49msample(\u001b[39m*\u001b[39;49margs, device \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice, use_tqdm \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_main, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    886\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:93\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m was_training \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining\n\u001b[1;32m     92\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 93\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     94\u001b[0m model\u001b[39m.\u001b[39mtrain(was_training)\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:2134\u001b[0m, in \u001b[0;36mImagen.sample\u001b[0;34m(self, texts, text_masks, text_embeds, cond_images, inpaint_images, inpaint_masks, inpaint_resample_times, batch_size, cond_scale, lowres_sample_noise_level, stop_at_unet_number, return_all_unet_outputs, return_pil_images, device, use_tqdm)\u001b[0m\n\u001b[1;32m   2130\u001b[0m         lowres_cond_img, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlowres_noise_schedule\u001b[39m.\u001b[39mq_sample(x_start \u001b[39m=\u001b[39m lowres_cond_img, t \u001b[39m=\u001b[39m lowres_noise_times, noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(lowres_cond_img))\n\u001b[1;32m   2132\u001b[0m     shape \u001b[39m=\u001b[39m (batch_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannels, image_size, image_size)\n\u001b[0;32m-> 2134\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample_loop(\n\u001b[1;32m   2135\u001b[0m         unet,\n\u001b[1;32m   2136\u001b[0m         shape,\n\u001b[1;32m   2137\u001b[0m         text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2138\u001b[0m         text_mask \u001b[39m=\u001b[39;49m text_masks,\n\u001b[1;32m   2139\u001b[0m         cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2140\u001b[0m         inpaint_images \u001b[39m=\u001b[39;49m inpaint_images,\n\u001b[1;32m   2141\u001b[0m         inpaint_masks \u001b[39m=\u001b[39;49m inpaint_masks,\n\u001b[1;32m   2142\u001b[0m         inpaint_resample_times \u001b[39m=\u001b[39;49m inpaint_resample_times,\n\u001b[1;32m   2143\u001b[0m         cond_scale \u001b[39m=\u001b[39;49m unet_cond_scale,\n\u001b[1;32m   2144\u001b[0m         lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2145\u001b[0m         lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2146\u001b[0m         noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2147\u001b[0m         pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2148\u001b[0m         dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold,\n\u001b[1;32m   2149\u001b[0m         use_tqdm \u001b[39m=\u001b[39;49m use_tqdm\n\u001b[1;32m   2150\u001b[0m     )\n\u001b[1;32m   2152\u001b[0m     outputs\u001b[39m.\u001b[39mappend(img)\n\u001b[1;32m   2154\u001b[0m \u001b[39mif\u001b[39;00m exists(stop_at_unet_number) \u001b[39mand\u001b[39;00m stop_at_unet_number \u001b[39m==\u001b[39m unet_number:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:2030\u001b[0m, in \u001b[0;36mImagen.p_sample_loop\u001b[0;34m(self, unet, shape, noise_scheduler, lowres_cond_img, lowres_noise_times, text_embeds, text_mask, cond_images, inpaint_images, inpaint_masks, inpaint_resample_times, cond_scale, pred_objective, dynamic_threshold, use_tqdm)\u001b[0m\n\u001b[1;32m   2027\u001b[0m     noised_inpaint_images, _ \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mq_sample(inpaint_images, t \u001b[39m=\u001b[39m times)\n\u001b[1;32m   2028\u001b[0m     img \u001b[39m=\u001b[39m img \u001b[39m*\u001b[39m \u001b[39m~\u001b[39minpaint_masks \u001b[39m+\u001b[39m noised_inpaint_images \u001b[39m*\u001b[39m inpaint_masks\n\u001b[0;32m-> 2030\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_sample(\n\u001b[1;32m   2031\u001b[0m     unet,\n\u001b[1;32m   2032\u001b[0m     img,\n\u001b[1;32m   2033\u001b[0m     times,\n\u001b[1;32m   2034\u001b[0m     t_next \u001b[39m=\u001b[39;49m times_next,\n\u001b[1;32m   2035\u001b[0m     text_embeds \u001b[39m=\u001b[39;49m text_embeds,\n\u001b[1;32m   2036\u001b[0m     text_mask \u001b[39m=\u001b[39;49m text_mask,\n\u001b[1;32m   2037\u001b[0m     cond_images \u001b[39m=\u001b[39;49m cond_images,\n\u001b[1;32m   2038\u001b[0m     cond_scale \u001b[39m=\u001b[39;49m cond_scale,\n\u001b[1;32m   2039\u001b[0m     lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img,\n\u001b[1;32m   2040\u001b[0m     lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times,\n\u001b[1;32m   2041\u001b[0m     noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler,\n\u001b[1;32m   2042\u001b[0m     pred_objective \u001b[39m=\u001b[39;49m pred_objective,\n\u001b[1;32m   2043\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold\n\u001b[1;32m   2044\u001b[0m )\n\u001b[1;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m has_inpainting \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_last_resample_step \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39mall(is_last_timestep)):\n\u001b[1;32m   2047\u001b[0m     renoised_img \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mq_sample_from_to(img, times_next, times)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1973\u001b[0m, in \u001b[0;36mImagen.p_sample\u001b[0;34m(self, unet, x, t, noise_scheduler, t_next, text_embeds, text_mask, cond_images, cond_scale, lowres_cond_img, lowres_noise_times, pred_objective, dynamic_threshold)\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[1;32m   1955\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_sample\u001b[39m(\n\u001b[1;32m   1956\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1970\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m ):\n\u001b[1;32m   1972\u001b[0m     b, \u001b[39m*\u001b[39m_, device \u001b[39m=\u001b[39m \u001b[39m*\u001b[39mx\u001b[39m.\u001b[39mshape, x\u001b[39m.\u001b[39mdevice\n\u001b[0;32m-> 1973\u001b[0m     model_mean, _, model_log_variance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp_mean_variance(unet, x \u001b[39m=\u001b[39;49m x, t \u001b[39m=\u001b[39;49m t, t_next \u001b[39m=\u001b[39;49m t_next, noise_scheduler \u001b[39m=\u001b[39;49m noise_scheduler, text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_mask, cond_images \u001b[39m=\u001b[39;49m cond_images, cond_scale \u001b[39m=\u001b[39;49m cond_scale, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_noise_times \u001b[39m=\u001b[39;49m lowres_noise_times, pred_objective \u001b[39m=\u001b[39;49m pred_objective, dynamic_threshold \u001b[39m=\u001b[39;49m dynamic_threshold)\n\u001b[1;32m   1974\u001b[0m     noise \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(x)\n\u001b[1;32m   1975\u001b[0m     \u001b[39m# no noise when t == 0\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1928\u001b[0m, in \u001b[0;36mImagen.p_mean_variance\u001b[0;34m(self, unet, x, t, noise_scheduler, text_embeds, text_mask, cond_images, lowres_cond_img, lowres_noise_times, cond_scale, model_output, t_next, pred_objective, dynamic_threshold)\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\n\u001b[1;32m   1909\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1910\u001b[0m     unet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m ):\n\u001b[1;32m   1926\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (cond_scale \u001b[39m!=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_classifier_guidance), \u001b[39m'\u001b[39m\u001b[39mimagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1928\u001b[0m     pred \u001b[39m=\u001b[39m default(model_output, \u001b[39mlambda\u001b[39;49;00m: unet\u001b[39m.\u001b[39;49mforward_with_cond_scale(x, noise_scheduler\u001b[39m.\u001b[39;49mget_condition(t), text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_mask, cond_images \u001b[39m=\u001b[39;49m cond_images, cond_scale \u001b[39m=\u001b[39;49m cond_scale, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_noise_times \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowres_noise_schedule\u001b[39m.\u001b[39;49mget_condition(lowres_noise_times)))\n\u001b[1;32m   1930\u001b[0m     \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1931\u001b[0m         x_start \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mpredict_start_from_noise(x, t \u001b[39m=\u001b[39m t, noise \u001b[39m=\u001b[39m pred)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:63\u001b[0m, in \u001b[0;36mdefault\u001b[0;34m(val, d)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m exists(val):\n\u001b[1;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m val\n\u001b[0;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m d() \u001b[39mif\u001b[39;00m callable(d) \u001b[39melse\u001b[39;00m d\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1928\u001b[0m, in \u001b[0;36mImagen.p_mean_variance.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1908\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mp_mean_variance\u001b[39m(\n\u001b[1;32m   1909\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1910\u001b[0m     unet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1924\u001b[0m     dynamic_threshold \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m ):\n\u001b[1;32m   1926\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m (cond_scale \u001b[39m!=\u001b[39m \u001b[39m1.\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcan_classifier_guidance), \u001b[39m'\u001b[39m\u001b[39mimagen was not trained with conditional dropout, and thus one cannot use classifier free guidance (cond_scale anything other than 1)\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1928\u001b[0m     pred \u001b[39m=\u001b[39m default(model_output, \u001b[39mlambda\u001b[39;00m: unet\u001b[39m.\u001b[39;49mforward_with_cond_scale(x, noise_scheduler\u001b[39m.\u001b[39;49mget_condition(t), text_embeds \u001b[39m=\u001b[39;49m text_embeds, text_mask \u001b[39m=\u001b[39;49m text_mask, cond_images \u001b[39m=\u001b[39;49m cond_images, cond_scale \u001b[39m=\u001b[39;49m cond_scale, lowres_cond_img \u001b[39m=\u001b[39;49m lowres_cond_img, lowres_noise_times \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlowres_noise_schedule\u001b[39m.\u001b[39;49mget_condition(lowres_noise_times)))\n\u001b[1;32m   1930\u001b[0m     \u001b[39mif\u001b[39;00m pred_objective \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnoise\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1931\u001b[0m         x_start \u001b[39m=\u001b[39m noise_scheduler\u001b[39m.\u001b[39mpredict_start_from_noise(x, t \u001b[39m=\u001b[39m t, noise \u001b[39m=\u001b[39m pred)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1438\u001b[0m, in \u001b[0;36mUnet.forward_with_cond_scale\u001b[0;34m(self, cond_scale, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_with_cond_scale\u001b[39m(\n\u001b[1;32m   1433\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1434\u001b[0m     \u001b[39m*\u001b[39margs,\n\u001b[1;32m   1435\u001b[0m     cond_scale \u001b[39m=\u001b[39m \u001b[39m1.\u001b[39m,\n\u001b[1;32m   1436\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   1437\u001b[0m ):\n\u001b[0;32m-> 1438\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1440\u001b[0m     \u001b[39mif\u001b[39;00m cond_scale \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1441\u001b[0m         \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:1596\u001b[0m, in \u001b[0;36mUnet.forward\u001b[0;34m(self, x, time, lowres_cond_img, lowres_noise_times, text_embeds, text_mask, cond_images, cond_drop_prob)\u001b[0m\n\u001b[1;32m   1593\u001b[0m     x \u001b[39m=\u001b[39m resnet_block(x, t)\n\u001b[1;32m   1594\u001b[0m     hiddens\u001b[39m.\u001b[39mappend(x)\n\u001b[0;32m-> 1596\u001b[0m x \u001b[39m=\u001b[39m attn_block(x, c)\n\u001b[1;32m   1597\u001b[0m hiddens\u001b[39m.\u001b[39mappend(x)\n\u001b[1;32m   1599\u001b[0m \u001b[39mif\u001b[39;00m exists(post_downsample):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:989\u001b[0m, in \u001b[0;36mLinearAttentionTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, context \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    988\u001b[0m     \u001b[39mfor\u001b[39;00m attn, ff \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 989\u001b[0m         x \u001b[39m=\u001b[39m attn(x, context \u001b[39m=\u001b[39;49m context) \u001b[39m+\u001b[39m x\n\u001b[1;32m    990\u001b[0m         x \u001b[39m=\u001b[39m ff(x) \u001b[39m+\u001b[39m x\n\u001b[1;32m    991\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/imagen_pytorch/imagen_pytorch.py:889\u001b[0m, in \u001b[0;36mLinearAttention.forward\u001b[0;34m(self, fmap, context)\u001b[0m\n\u001b[1;32m    886\u001b[0m out \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39m\u001b[39mb n d, b d e -> b n e\u001b[39m\u001b[39m'\u001b[39m, q, context)\n\u001b[1;32m    887\u001b[0m out \u001b[39m=\u001b[39m rearrange(out, \u001b[39m'\u001b[39m\u001b[39m(b h) (x y) d -> b (h d) x y\u001b[39m\u001b[39m'\u001b[39m, h \u001b[39m=\u001b[39m h, x \u001b[39m=\u001b[39m x, y \u001b[39m=\u001b[39m y)\n\u001b[0;32m--> 889\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnonlin(out)\n\u001b[1;32m    890\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_out(out)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/modules/activation.py:391\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 391\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.3/lib/python3.10/site-packages/torch/nn/functional.py:2048\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   2047\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39msilu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m-> 2048\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49msilu(\u001b[39minput\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# unets for unconditional imagen\n",
    "\n",
    "unet1 = Unet(\n",
    "    dim = 32,\n",
    "    dim_mults = (1, 2, 4),\n",
    "    num_resnet_blocks = 3,\n",
    "    layer_attns = (False, True, True),\n",
    "    layer_cross_attns = False,\n",
    "    use_linear_attn = True\n",
    ")\n",
    "\n",
    "unet2 = SRUnet256(\n",
    "    dim = 32,\n",
    "    dim_mults = (1, 2, 4),\n",
    "    num_resnet_blocks = (2, 4, 8),\n",
    "    layer_attns = (False, False, True),\n",
    "    layer_cross_attns = False\n",
    ")\n",
    "\n",
    "# imagen, which contains the unets above (base unet and super resoluting ones)\n",
    "\n",
    "imagen = Imagen(\n",
    "    condition_on_text = False,   # this must be set to False for unconditional Imagen\n",
    "    unets = (unet1, unet2),\n",
    "    image_sizes = (64, 128),\n",
    "    timesteps = 1000\n",
    ")\n",
    "\n",
    "trainer = ImagenTrainer(imagen)\n",
    "\n",
    "# now get a ton of images and feed it through the Imagen trainer\n",
    "\n",
    "training_images = torch.randn(4, 3, 256, 256)\n",
    "\n",
    "# train each unet separately\n",
    "# in this example, only training on unet number 1\n",
    "\n",
    "loss = trainer(training_images, unet_number = 1)\n",
    "trainer.update(unet_number = 1)\n",
    "\n",
    "# do the above for many many many many steps\n",
    "# now you can sample images unconditionally from the cascading unet(s)\n",
    "\n",
    "images = trainer.sample(batch_size = 16) # (16, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit ('3.10.3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "10626171d4353dd8d0f12b0dae77464b904fee8f635bb045a55f368206a04bde"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
