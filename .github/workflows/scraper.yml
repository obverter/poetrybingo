# Call this whatever you want
name: scraper

# When does it get run?
on:
  # workflow_dispatch means "I can click a button and force it to run"
  workflow_dispatch:
  # schedule/cron means "on a set schedule"
  schedule:
    - cron: '*/5 * * * *'

# Add permissions for the workflow
permissions:
  contents: write
  issues: write

jobs:
  scrape:
    # For some reason, we run love versions of linux
    runs-on: ubuntu-latest
    timeout-minutes: 5  # Prevent hanging jobs
    
    # Add caching for pip and Python
    env:
      PIP_CACHE_DIR: ~/.cache/pip
      PYTHONPATH: ${{ github.workspace }}
    
    steps:
      # Download all of the code from your repo
      - name: Check out this repo
        uses: actions/checkout@v3
        with:
          fetch-depth: 1  # Only fetch the latest commit
          token: ${{ secrets.GITHUB_TOKEN }}
          ref: main  # Explicitly checkout main branch
      # Set up Python 3.10
      - name: Set up Python
        uses: actions/setup-python@v4  # Updated to latest version
        with:
          python-version: '3.10'
          cache: 'pip'  # Enable pip caching
          cache-dependency-path: |
            automatic-scraper/requirements.txt
            **/*.py
      - name: Cache pip packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r automatic-scraper/requirements.txt
      # MAKE SURE YOUR SCRAPER FILENAME MATCHES THE FILENAME HERE!!
      - name: Run the scraping script
        id: scrape
        run: |
          python automatic-scraper/scraper.py || exit 1
      # I just stole the rest of this code so don't ask me questions about it
      - name: Check for changes
        id: check_changes
        run: |
          if [[ -n $(git status --porcelain) ]]; then
            echo "Changes detected"
            echo "changes=true" >> $GITHUB_OUTPUT
          else
            echo "No changes detected"
            echo "changes=false" >> $GITHUB_OUTPUT
          fi
      - name: Commit and push if content changed
        if: steps.check_changes.outputs.changes
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add headlines.csv headlines.json
          timestamp=$(date -u)
          git commit -m "Update headlines: ${timestamp}"
          git push origin main:main
      - name: Clean up old logs
        if: always()
        run: |
          find logs -type f -name "scraper_*.log" -mtime +7 -delete
      - name: Notify on failure
        if: failure()
        uses: actions/github-script@v6
        with:
          script: |
            const workflow_run_url = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const message = `‚ùå Scraper workflow failed. Please check the [workflow run](${workflow_run_url}) for details.`;
            
            // Create a new issue if the workflow failed
            if (context.eventName === 'workflow_dispatch' || context.eventName === 'schedule') {
              github.rest.issues.create({
                owner: context.repo.owner,
                repo: context.repo.repo,
                title: 'Scraper Workflow Failed',
                body: message,
                labels: ['bug', 'scraper']
              });
            }
            
            // Log the error message
            console.log(message);
